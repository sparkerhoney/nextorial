{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4159,"status":"ok","timestamp":1698603385873,"user":{"displayName":"Honey Jeong","userId":"03929989360297315294"},"user_tz":-540},"id":"aBN38mPBuvUv","outputId":"81d95680-3a16-4107-bf95-ef2c08114ade"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting pytorch-tabnet\n","  Downloading pytorch_tabnet-4.1.0-py3-none-any.whl (44 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (1.23.5)\n","Requirement already satisfied: scikit_learn\u003e0.21 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (1.2.2)\n","Requirement already satisfied: scipy\u003e1.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (1.11.3)\n","Requirement already satisfied: torch\u003e=1.3 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (2.1.0+cu118)\n","Requirement already satisfied: tqdm\u003e=4.36 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (4.66.1)\n","Requirement already satisfied: joblib\u003e=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit_learn\u003e0.21-\u003epytorch-tabnet) (1.3.2)\n","Requirement already satisfied: threadpoolctl\u003e=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn\u003e0.21-\u003epytorch-tabnet) (3.2.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.3-\u003epytorch-tabnet) (3.12.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.3-\u003epytorch-tabnet) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.3-\u003epytorch-tabnet) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.3-\u003epytorch-tabnet) (3.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.3-\u003epytorch-tabnet) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.3-\u003epytorch-tabnet) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.3-\u003epytorch-tabnet) (2.1.0)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-\u003etorch\u003e=1.3-\u003epytorch-tabnet) (2.1.3)\n","Requirement already satisfied: mpmath\u003e=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-\u003etorch\u003e=1.3-\u003epytorch-tabnet) (1.3.0)\n","Installing collected packages: pytorch-tabnet\n","Successfully installed pytorch-tabnet-4.1.0\n"]}],"source":["! pip install pytorch-tabnet"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5897,"status":"ok","timestamp":1698603391758,"user":{"displayName":"Honey Jeong","userId":"03929989360297315294"},"user_tz":-540},"id":"IpKn2mzLOkJc","outputId":"4ad9c21d-daf4-436e-e711-9c59947824b7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting optuna\n","  Downloading optuna-3.4.0-py3-none-any.whl (409 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/409.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m399.4/409.6 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.6/409.6 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting alembic\u003e=1.5.0 (from optuna)\n","  Downloading alembic-1.12.1-py3-none-any.whl (226 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.8/226.8 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting colorlog (from optuna)\n","  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.23.5)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.2)\n","Requirement already satisfied: sqlalchemy\u003e=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.22)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n","Collecting Mako (from alembic\u003e=1.5.0-\u003eoptuna)\n","  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions\u003e=4 in /usr/local/lib/python3.10/dist-packages (from alembic\u003e=1.5.0-\u003eoptuna) (4.5.0)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy\u003e=1.3.0-\u003eoptuna) (3.0.0)\n","Requirement already satisfied: MarkupSafe\u003e=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako-\u003ealembic\u003e=1.5.0-\u003eoptuna) (2.1.3)\n","Installing collected packages: Mako, colorlog, alembic, optuna\n","Successfully installed Mako-1.2.4 alembic-1.12.1 colorlog-6.7.0 optuna-3.4.0\n"]}],"source":["! pip install optuna"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":669,"status":"ok","timestamp":1698603392419,"user":{"displayName":"Honey Jeong","userId":"03929989360297315294"},"user_tz":-540},"id":"SWrLKXATvQ3L"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","import pandas as pd\n","import numpy as np"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":5755,"status":"ok","timestamp":1698603454652,"user":{"displayName":"Honey Jeong","userId":"03929989360297315294"},"user_tz":-540},"id":"DJVHmzp3wXUl"},"outputs":[],"source":["match_data = pd.read_csv('/content/drive/MyDrive/nextorial/data/match_data.csv')\n","test_data = pd.read_csv('/content/drive/MyDrive/nextorial/data/test_data.csv')"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1698603454655,"user":{"displayName":"Honey Jeong","userId":"03929989360297315294"},"user_tz":-540},"id":"d9IDB-y-vpZc"},"outputs":[],"source":["class DataProcessor:\n","\n","    def __init__(self, df):\n","        self.data = df\n","\n","    @staticmethod\n","    # def convert_tier_to_numeric(tier):\n","    #     tier_dict = {\n","    #         'unranked': 0,\n","    #         'bronze': 1,\n","    #         'silver': 2,\n","    #         'gold': 3,\n","    #         'platinum': 4,\n","    #         'diamond': 5,\n","    #         'master': 6\n","    #     }\n","    #     return tier_dict.get(tier, -1)\n","\n","    @staticmethod\n","    def convert_tier_to_exponential_weight(tier):\n","        tier_dict = {\n","            'unranked': 0,\n","            'bronze': 1,\n","            'silver': 2,\n","            'gold': 3,\n","            'platinum': 4,\n","            'diamond': 5,\n","            'master': 6\n","        }\n","        tier_numeric = tier_dict.get(tier, -1)\n","        return np.exp(tier_numeric)\n","\n","    def add_tier_exponential_weight(self):\n","        self.data['tier_exp_weight'] = self.data['tier'].apply(self.convert_tier_to_exponential_weight)\n","        return self.data\n","\n","    # def calculate_team_features(self):\n","    #     return self.data.groupby(['matchid', 'teamid']).agg({\n","    #         'mmr': 'mean',\n","    #         'winstreak': 'mean',\n","    #         'losestreak': 'mean',\n","    #         'recentwinprob': 'mean'\n","    #     }).reset_index()\n","\n","    def preprocess(self):\n","        # match_team_features = self.calculate_team_features()\n","        # self.data = self.data.merge(match_team_features, on=['matchid', 'teamid'], suffixes=('', '_team_avg'))\n","        self.data = self.normalize_column('accumatches')\n","        self.data = self.compute_team_stats()\n","        self.data = self.compute_recent_performance_index()\n","        self.data = self.process_guild_info(202068.571428571428400)\n","        self.data = self.compute_mmr_diff_and_variance()\n","        self.data = self.compute_recent_winprob_stats()\n","        self.data = self.apply_tier_conversion_and_compute_average()\n","        self.data = self.compute_streak_rate()\n","        self.data = self.guild_mean()\n","        self.data = self.guild_median()\n","        self.data = self.guild_mode()\n","        self.data = self.add_tier_exponential_weight()\n","\n","    # guild_membership\n","    def guild_mean(self):\n","        df = self.data.copy()\n","        df['guildid'].fillna('NoGuild', inplace=True)\n","        df['tier_numeric'] = df['tier'].map(self.convert_tier_to_exponential_weight)\n","        guild_mean = df.groupby('guildid')[['mmr', 'winstreak', 'recentwinprob', 'accumatches', 'tier_numeric']].mean()\n","        new_columns = {col: f'{col}guild_mean' for col in guild_mean.columns}\n","        guild_mean.rename(columns=new_columns, inplace=True)\n","        df = pd.merge(df, guild_mean, on='guildid', how='left')\n","        return df\n","\n","    def guild_median(self):\n","        df = self.data.copy()\n","        df['guildid'].fillna('NoGuild', inplace=True)\n","        df['tier_numeric'] = df['tier'].map(self.convert_tier_to_exponential_weight)\n","        guild_median = df.groupby('guildid')[['mmr', 'winstreak', 'recentwinprob', 'accumatches', 'tier_numeric']].median()\n","        new_columns = {col: f'{col}guild_median' for col in guild_median.columns}\n","        guild_median.rename(columns=new_columns, inplace=True)\n","        df = pd.merge(df, guild_median, on='guildid', how='left')\n","        return df\n","\n","    def guild_mode(self):\n","        df = self.data.copy()\n","        df['guildid'].fillna('NoGuild', inplace=True)\n","        df['tier_numeric'] = df['tier'].map(self.convert_tier_to_exponential_weight)\n","\n","        def calculate_mode(group):\n","            return group.mode().iloc[0]\n","\n","        guild_mode = df.groupby('guildid')[['mmr', 'winstreak', 'recentwinprob', 'accumatches', 'tier_numeric']].apply(calculate_mode)\n","        new_columns = {col: f'{col}guild_mode' for col in guild_mode.columns}\n","        guild_mode.rename(columns=new_columns, inplace=True)\n","        df = pd.merge(df, guild_mode, on='guildid', how='left')\n","        return df\n","\n","    def normalize_column(self, column):\n","        self.data[f'normalized_{column}'] = (self.data[column] - self.data[column].min()) / (self.data[column].max() - self.data[column].min())\n","        return self.data\n","\n","    def compute_team_stats(self):\n","        grouped = self.data.groupby(['matchid', 'teamid'])\n","        self.data['team_max_accumatches'] = grouped['accumatches'].transform('max')\n","        self.data['team_min_accumatches'] = grouped['accumatches'].transform('min')\n","        self.data['accumatches_diff'] = self.data['team_max_accumatches'] - self.data['team_min_accumatches']\n","        self.data['accumatches_variance'] = grouped['accumatches'].transform('var')\n","        return self.data\n","\n","    def compute_recent_performance_index(self):\n","        self.data['recent_performance_index'] = self.data['winstreak'] * self.data['recentwinprob']\n","        return self.data\n","\n","    def process_guild_info(self, threshold):\n","        guild_mean_mmr = self.data.groupby('guildid')['mmr'].mean()\n","        self.data['guild_mean_mmr'] = self.data['guildid'].map(guild_mean_mmr)\n","        self.data['high_mmr_guild'] = (self.data['guild_mean_mmr'] \u003e threshold).astype(int)\n","        return self.data\n","\n","    def compute_mmr_diff_and_variance(self):\n","        mmr_diff_grouped = self.data.groupby('teamid')['mmr'].agg(['max', 'min'])\n","        self.data['mmr_diff'] = self.data['teamid'].map(mmr_diff_grouped['max'] - mmr_diff_grouped['min'])\n","        mmr_variance_grouped = self.data.groupby('teamid')['mmr'].var()\n","        self.data['mmr_variance'] = self.data['teamid'].map(mmr_variance_grouped)\n","        return self.data\n","\n","    def compute_recent_winprob_stats(self):\n","        grouped = self.data.groupby('matchid')\n","        self.data['recentwinprob_max'] = grouped['recentwinprob'].transform('max')\n","        self.data['recentwinprob_min'] = grouped['recentwinprob'].transform('min')\n","        self.data['recentwinprob_diff'] = self.data['recentwinprob_max'] - self.data['recentwinprob_min']\n","        self.data['recentwinprob_mean'] = grouped['recentwinprob'].transform('mean')\n","        self.data['recentwinprob_diff_from_mean'] = (self.data['recentwinprob'] - self.data['recentwinprob_mean'])**2\n","        self.data['recentwinprob_variance'] = grouped['recentwinprob_diff_from_mean'].transform('mean')\n","        return self.data\n","\n","    def apply_tier_conversion_and_compute_average(self):\n","        self.data['tier_numeric'] = self.data['tier'].apply(self.convert_tier_to_exponential_weight)\n","        average_tier = self.data.groupby(['matchid', 'teamid'])['tier_numeric'].mean().reset_index()\n","        average_tier.rename(columns={'tier_numeric': 'average_tier'}, inplace=True)\n","        self.data = self.data.merge(average_tier, on=['matchid', 'teamid'])\n","        return self.data\n","\n","    @staticmethod\n","    def calculate_streak_rate(row):\n","        winstreak, losestreak = row['winstreak'], row['losestreak']\n","        if winstreak + losestreak == 0:\n","            return 0\n","        return winstreak / (winstreak + losestreak)\n","\n","    def compute_streak_rate(self):\n","        self.data['streak_rate'] = self.data.apply(self.calculate_streak_rate, axis=1)\n","        return self.data"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":15856,"status":"ok","timestamp":1698603470497,"user":{"displayName":"Honey Jeong","userId":"03929989360297315294"},"user_tz":-540},"id":"Rtq23JFVwEt4"},"outputs":[],"source":["processor = DataProcessor(match_data)\n","processor.preprocess()\n","processed_data = processor.data"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1698603470498,"user":{"displayName":"Honey Jeong","userId":"03929989360297315294"},"user_tz":-540},"id":"wvSF3Jvv6Rbq"},"outputs":[],"source":["# # Processed data is already defined in the context, we'll just export it to a CSV file\n","# processed_data.to_csv('/content/drive/MyDrive/nextorial/data/processed_match_data.csv', index=False)"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":1996,"status":"ok","timestamp":1698603472484,"user":{"displayName":"Honey Jeong","userId":"03929989360297315294"},"user_tz":-540},"id":"Y_yZXSVm4_Gq"},"outputs":[],"source":["from sklearn.pipeline import Pipeline\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.model_selection import train_test_split\n","\n","# 데이터에서 특성과 타겟 변수를 분리합니다.\n","target = 'matchresult'\n","# Drop identifiers and the original 'tier' feature\n","features_to_drop = ['createdatekst', 'matchid', 'accountid', 'guildid', 'tier', 'matchscore', 'isDrop', 'isEscape']\n","features = processed_data.columns.drop([target] + features_to_drop)\n","X = processed_data[features]\n","y = processed_data[target]\n","\n","# 수치형 및 범주형 데이터를 위한 변환기 정의\n","numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n","categorical_features = X.select_dtypes(include=['object']).columns\n","\n","# 순서형 특성은 이미 'tier_numeric'으로 변환되어 있으므로 여기서는 처리하지 않습니다.\n","\n","# 수치형 및 범주형 데이터에 대한 파이프라인을 만듭니다.\n","numeric_transformer = Pipeline(steps=[\n","    ('imputer', SimpleImputer(strategy='median'))  # NaN 값을 중앙값으로 대체\n","])\n","\n","categorical_transformer = Pipeline(steps=[\n","    ('imputer', SimpleImputer(strategy='most_frequent')),  # NaN 값을 최빈값으로 대체\n","    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # 원-핫 인코딩\n","])\n","\n","# ColumnTransformer를 생성하여 변환기를 결합합니다.\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', numeric_transformer, numeric_features),\n","        ('cat', categorical_transformer, categorical_features)\n","    ],\n","    remainder='passthrough'  # 나머지 열은 변경하지 않고 유지\n",")\n","\n","# 전처리를 특성 데이터에 적용합니다.\n","X_encoded = preprocessor.fit_transform(X)\n","\n","# 데이터셋을 훈련 세트와 검증/테스트 세트로 분할합니다.\n","X_temp, X_test, y_temp, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n","X_train, X_valid, y_train, y_valid = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2\n"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":4935,"status":"ok","timestamp":1698603477414,"user":{"displayName":"Honey Jeong","userId":"03929989360297315294"},"user_tz":-540},"id":"zkjaWTV75tHN"},"outputs":[],"source":["import torch, gc\n","\n","gc.collect()\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":52,"status":"ok","timestamp":1698603477417,"user":{"displayName":"Honey Jeong","userId":"03929989360297315294"},"user_tz":-540},"id":"_5GHgZ9eQ7NR"},"outputs":[],"source":["# from pytorch_tabnet.tab_model import TabNetClassifier\n","\n","# clf = TabNetClassifier()\n","# clf.fit(\n","#     X_train, y_train,\n","#     eval_set=[(X_valid, y_valid)],\n","#     batch_size = 8,\n","#     max_epochs = 2,\n","# )\n","# preds = clf.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"esRGsRfDJwZD"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n","  warnings.warn(f\"Device used : {self.device}\")\n"]},{"name":"stdout","output_type":"stream","text":["epoch 0  | loss: 0.70116 | val_0_auc: 0.52699 |  0:00:12s\n","epoch 1  | loss: 0.69155 | val_0_auc: 0.5271  |  0:00:23s\n","epoch 2  | loss: 0.69164 | val_0_auc: 0.53329 |  0:00:33s\n","epoch 3  | loss: 0.69115 | val_0_auc: 0.53797 |  0:00:43s\n","epoch 4  | loss: 0.69035 | val_0_auc: 0.54108 |  0:00:53s\n","epoch 5  | loss: 0.68978 | val_0_auc: 0.53964 |  0:01:04s\n","epoch 6  | loss: 0.68956 | val_0_auc: 0.53978 |  0:01:14s\n","epoch 7  | loss: 0.68957 | val_0_auc: 0.54851 |  0:01:24s\n","epoch 8  | loss: 0.68865 | val_0_auc: 0.54703 |  0:01:34s\n","epoch 9  | loss: 0.6888  | val_0_auc: 0.55046 |  0:01:45s\n","epoch 10 | loss: 0.68846 | val_0_auc: 0.54708 |  0:01:55s\n","epoch 11 | loss: 0.6888  | val_0_auc: 0.54957 |  0:02:05s\n","epoch 12 | loss: 0.68821 | val_0_auc: 0.5498  |  0:02:15s\n","epoch 13 | loss: 0.68778 | val_0_auc: 0.55229 |  0:02:25s\n","epoch 14 | loss: 0.68764 | val_0_auc: 0.54907 |  0:02:35s\n","epoch 15 | loss: 0.68798 | val_0_auc: 0.54547 |  0:02:46s\n","epoch 16 | loss: 0.68873 | val_0_auc: 0.53915 |  0:02:56s\n","epoch 17 | loss: 0.68878 | val_0_auc: 0.53738 |  0:03:06s\n","epoch 18 | loss: 0.68816 | val_0_auc: 0.54527 |  0:03:16s\n","epoch 19 | loss: 0.68747 | val_0_auc: 0.55172 |  0:03:26s\n","epoch 20 | loss: 0.68776 | val_0_auc: 0.54639 |  0:03:36s\n","epoch 21 | loss: 0.6877  | val_0_auc: 0.55097 |  0:03:46s\n","epoch 22 | loss: 0.68772 | val_0_auc: 0.5473  |  0:03:57s\n","epoch 23 | loss: 0.68864 | val_0_auc: 0.54587 |  0:04:07s\n","\n","Early stopping occurred at epoch 23 with best_epoch = 13 and best_val_0_auc = 0.55229\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n","  warnings.warn(wrn_msg)\n"]}],"source":["from pytorch_tabnet.tab_model import TabNetClassifier\n","\n","clf = TabNetClassifier()\n","clf.fit(\n","    X_train, y_train,\n","    eval_set=[(X_valid, y_valid)],\n","    batch_size = 1024,\n","    max_epochs = 50,\n",")\n","preds = clf.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Ao3oWNvbKkRE"},"outputs":[],"source":["# import optuna\n","# from pytorch_tabnet.tab_model import TabNetClassifier\n","# from sklearn.metrics import accuracy_score\n","\n","# # Optuna를 위한 목적 함수 정의\n","# def objective(trial):\n","\n","#     # 탐색할 하이퍼파라미터 설정\n","#     param = {\n","#         'max_epochs': trial.suggest_int('max_epochs', 10, 30),\n","#         'batch_size': trial.suggest_categorical('batch_size', [256, 512, 1024]),\n","#         'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1)\n","#     }\n","\n","#     # 모델 생성\n","#     model = TabNetClassifier(optimizer_params=dict(lr=param['learning_rate']))\n","\n","#     # 모델 학습\n","#     model.fit(\n","#         X_train, y_train,\n","#         eval_set=[(X_valid, y_valid)],\n","#         eval_name=['valid'],\n","#         eval_metric=['accuracy'],\n","#         max_epochs=param['max_epochs'],\n","#         batch_size=param['batch_size'],\n","#         patience=5,\n","#         virtual_batch_size=128,\n","#         num_workers=0,\n","#         drop_last=False\n","#     )\n","\n","#     # 검증 데이터에 대한 예측 및 정확도 측정\n","#     preds = model.predict(X_valid)\n","#     acc = accuracy_score(y_valid, preds)\n","#     return acc\n","\n","# # Optuna study 객체 생성\n","# study = optuna.create_study(direction='maximize', study_name='TabNet optimization')\n","\n","# # 목적 함수에 대한 최적화 실행\n","# study.optimize(objective, n_trials=20)  # n_trials는 시도할 횟수입니다.\n","\n","# # 최적의 하이퍼파라미터 출력\n","# print(f\"Best trial: {study.best_trial.params}\")\n","# print(f\"Best validation accuracy: {study.best_trial.value}\")\n","\n","# # 최적의 하이퍼파라미터를 사용하여 최종 모델 학습\n","# best_params = study.best_trial.params\n","# best_model = TabNetClassifier(optimizer_params=dict(lr=best_params['learning_rate']))\n","\n","# best_model.fit(\n","#     X_train, y_train,\n","#     eval_set=[(X_valid, y_valid)],\n","#     max_epochs=best_params['max_epochs'],\n","#     batch_size=best_params['batch_size'],\n","#     patience=10,\n","#     virtual_batch_size=128,\n","#     num_workers=0,\n","#     drop_last=False\n","# )\n","\n","# # 테스트 데이터에 대한 최종 평가\n","# final_preds = best_model.predict(X_test)\n","# final_acc = accuracy_score(y_test, final_preds)\n","# print(f\"Test accuracy: {final_acc}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"2CbMBaIqOXFn"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNuHTddUOiIiidBJTss2A9u","machine_shape":"hm","mount_file_id":"1JEOY9M-hfys2ia719fC_T6QtQTXZcVP7","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}