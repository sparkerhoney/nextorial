{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5228,"status":"ok","timestamp":1698590007519,"user":{"displayName":"Honey Jeong","userId":"03929989360297315294"},"user_tz":-540},"id":"aBN38mPBuvUv","outputId":"d82e4f92-a22e-44f0-a1b0-3bfe8b36cd0f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting pytorch-tabnet\n","  Downloading pytorch_tabnet-4.1.0-py3-none-any.whl (44 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (1.23.5)\n","Requirement already satisfied: scikit_learn>0.21 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (1.2.2)\n","Requirement already satisfied: scipy>1.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (1.11.3)\n","Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (2.1.0+cu118)\n","Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (4.66.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (3.2.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (3.12.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (3.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (2.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3->pytorch-tabnet) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3->pytorch-tabnet) (1.3.0)\n","Installing collected packages: pytorch-tabnet\n","Successfully installed pytorch-tabnet-4.1.0\n"]}],"source":["! pip install pytorch-tabnet"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5644,"status":"ok","timestamp":1698596284780,"user":{"displayName":"Honey Jeong","userId":"03929989360297315294"},"user_tz":-540},"id":"IpKn2mzLOkJc","outputId":"8a3f3763-55d4-4736-81cd-2748f39c32b6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting optuna\n","  Downloading optuna-3.4.0-py3-none-any.whl (409 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.6/409.6 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n","  Downloading alembic-1.12.1-py3-none-any.whl (226 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.8/226.8 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting colorlog (from optuna)\n","  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.2)\n","Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.22)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n","Collecting Mako (from alembic>=1.5.0->optuna)\n","  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.5.0)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.0)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n","Installing collected packages: Mako, colorlog, alembic, optuna\n","Successfully installed Mako-1.2.4 alembic-1.12.1 colorlog-6.7.0 optuna-3.4.0\n"]}],"source":["! pip install optuna"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SWrLKXATvQ3L"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","import pandas as pd\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DJVHmzp3wXUl"},"outputs":[],"source":["match_data = pd.read_csv('/content/drive/MyDrive/nextorial/data/match_data.csv')\n","test_data = pd.read_csv('/content/drive/MyDrive/nextorial/data/test_data.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d9IDB-y-vpZc"},"outputs":[],"source":["class DataProcessor:\n","\n","    def __init__(self, df):\n","        self.data = df\n","\n","    @staticmethod\n","    # def convert_tier_to_numeric(tier):\n","    #     tier_dict = {\n","    #         'unranked': 0,\n","    #         'bronze': 1,\n","    #         'silver': 2,\n","    #         'gold': 3,\n","    #         'platinum': 4,\n","    #         'diamond': 5,\n","    #         'master': 6\n","    #     }\n","    #     return tier_dict.get(tier, -1)\n","\n","    @staticmethod\n","    def convert_tier_to_exponential_weight(tier):\n","        tier_dict = {\n","            'unranked': 0,\n","            'bronze': 1,\n","            'silver': 2,\n","            'gold': 3,\n","            'platinum': 4,\n","            'diamond': 5,\n","            'master': 6\n","        }\n","        tier_numeric = tier_dict.get(tier, -1)\n","        return np.exp(tier_numeric)\n","\n","    def add_tier_exponential_weight(self):\n","        self.data['tier_exp_weight'] = self.data['tier'].apply(self.convert_tier_to_exponential_weight)\n","        return self.data\n","\n","    # def calculate_team_features(self):\n","    #     return self.data.groupby(['matchid', 'teamid']).agg({\n","    #         'mmr': 'mean',\n","    #         'winstreak': 'mean',\n","    #         'losestreak': 'mean',\n","    #         'recentwinprob': 'mean'\n","    #     }).reset_index()\n","\n","    def preprocess(self):\n","        # match_team_features = self.calculate_team_features()\n","        # self.data = self.data.merge(match_team_features, on=['matchid', 'teamid'], suffixes=('', '_team_avg'))\n","        self.data = self.normalize_column('accumatches')\n","        self.data = self.compute_team_stats()\n","        self.data = self.compute_recent_performance_index()\n","        self.data = self.process_guild_info(202068.571428571428400)\n","        self.data = self.compute_mmr_diff_and_variance()\n","        self.data = self.compute_recent_winprob_stats()\n","        self.data = self.apply_tier_conversion_and_compute_average()\n","        self.data = self.compute_streak_rate()\n","        self.data = self.guild_mean()\n","        self.data = self.guild_median()\n","        self.data = self.guild_mode()\n","        self.data = self.add_tier_exponential_weight()\n","\n","    # guild_membership\n","    def guild_mean(self):\n","        df = self.data.copy()\n","        df['guildid'].fillna('NoGuild', inplace=True)\n","        df['tier_numeric'] = df['tier'].map(self.convert_tier_to_exponential_weight)\n","        guild_mean = df.groupby('guildid')[['mmr', 'winstreak', 'recentwinprob', 'accumatches', 'tier_numeric']].mean()\n","        new_columns = {col: f'{col}guild_mean' for col in guild_mean.columns}\n","        guild_mean.rename(columns=new_columns, inplace=True)\n","        df = pd.merge(df, guild_mean, on='guildid', how='left')\n","        return df\n","\n","    def guild_median(self):\n","        df = self.data.copy()\n","        df['guildid'].fillna('NoGuild', inplace=True)\n","        df['tier_numeric'] = df['tier'].map(self.convert_tier_to_exponential_weight)\n","        guild_median = df.groupby('guildid')[['mmr', 'winstreak', 'recentwinprob', 'accumatches', 'tier_numeric']].median()\n","        new_columns = {col: f'{col}guild_median' for col in guild_median.columns}\n","        guild_median.rename(columns=new_columns, inplace=True)\n","        df = pd.merge(df, guild_median, on='guildid', how='left')\n","        return df\n","\n","    def guild_mode(self):\n","        df = self.data.copy()\n","        df['guildid'].fillna('NoGuild', inplace=True)\n","        df['tier_numeric'] = df['tier'].map(self.convert_tier_to_exponential_weight)\n","\n","        def calculate_mode(group):\n","            return group.mode().iloc[0]\n","\n","        guild_mode = df.groupby('guildid')[['mmr', 'winstreak', 'recentwinprob', 'accumatches', 'tier_numeric']].apply(calculate_mode)\n","        new_columns = {col: f'{col}guild_mode' for col in guild_mode.columns}\n","        guild_mode.rename(columns=new_columns, inplace=True)\n","        df = pd.merge(df, guild_mode, on='guildid', how='left')\n","        return df\n","\n","    def normalize_column(self, column):\n","        self.data[f'normalized_{column}'] = (self.data[column] - self.data[column].min()) / (self.data[column].max() - self.data[column].min())\n","        return self.data\n","\n","    def compute_team_stats(self):\n","        grouped = self.data.groupby(['matchid', 'teamid'])\n","        self.data['team_max_accumatches'] = grouped['accumatches'].transform('max')\n","        self.data['team_min_accumatches'] = grouped['accumatches'].transform('min')\n","        self.data['accumatches_diff'] = self.data['team_max_accumatches'] - self.data['team_min_accumatches']\n","        self.data['accumatches_variance'] = grouped['accumatches'].transform('var')\n","        return self.data\n","\n","    def compute_recent_performance_index(self):\n","        self.data['recent_performance_index'] = self.data['winstreak'] * self.data['recentwinprob']\n","        return self.data\n","\n","    def process_guild_info(self, threshold):\n","        guild_mean_mmr = self.data.groupby('guildid')['mmr'].mean()\n","        self.data['guild_mean_mmr'] = self.data['guildid'].map(guild_mean_mmr)\n","        self.data['high_mmr_guild'] = (self.data['guild_mean_mmr'] > threshold).astype(int)\n","        return self.data\n","\n","    def compute_mmr_diff_and_variance(self):\n","        mmr_diff_grouped = self.data.groupby('teamid')['mmr'].agg(['max', 'min'])\n","        self.data['mmr_diff'] = self.data['teamid'].map(mmr_diff_grouped['max'] - mmr_diff_grouped['min'])\n","        mmr_variance_grouped = self.data.groupby('teamid')['mmr'].var()\n","        self.data['mmr_variance'] = self.data['teamid'].map(mmr_variance_grouped)\n","        return self.data\n","\n","    def compute_recent_winprob_stats(self):\n","        grouped = self.data.groupby('matchid')\n","        self.data['recentwinprob_max'] = grouped['recentwinprob'].transform('max')\n","        self.data['recentwinprob_min'] = grouped['recentwinprob'].transform('min')\n","        self.data['recentwinprob_diff'] = self.data['recentwinprob_max'] - self.data['recentwinprob_min']\n","        self.data['recentwinprob_mean'] = grouped['recentwinprob'].transform('mean')\n","        self.data['recentwinprob_diff_from_mean'] = (self.data['recentwinprob'] - self.data['recentwinprob_mean'])**2\n","        self.data['recentwinprob_variance'] = grouped['recentwinprob_diff_from_mean'].transform('mean')\n","        return self.data\n","\n","    def apply_tier_conversion_and_compute_average(self):\n","        self.data['tier_numeric'] = self.data['tier'].apply(self.convert_tier_to_exponential_weight)\n","        average_tier = self.data.groupby(['matchid', 'teamid'])['tier_numeric'].mean().reset_index()\n","        average_tier.rename(columns={'tier_numeric': 'average_tier'}, inplace=True)\n","        self.data = self.data.merge(average_tier, on=['matchid', 'teamid'])\n","        return self.data\n","\n","    @staticmethod\n","    def calculate_streak_rate(row):\n","        winstreak, losestreak = row['winstreak'], row['losestreak']\n","        if winstreak + losestreak == 0:\n","            return 0\n","        return winstreak / (winstreak + losestreak)\n","\n","    def compute_streak_rate(self):\n","        self.data['streak_rate'] = self.data.apply(self.calculate_streak_rate, axis=1)\n","        return self.data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rtq23JFVwEt4"},"outputs":[],"source":["processor = DataProcessor(match_data)\n","processor.preprocess()\n","processed_data = processor.data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wvSF3Jvv6Rbq"},"outputs":[],"source":["# # Processed data is already defined in the context, we'll just export it to a CSV file\n","# processed_data.to_csv('/content/drive/MyDrive/nextorial/data/processed_match_data.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y_yZXSVm4_Gq"},"outputs":[],"source":["from sklearn.pipeline import Pipeline\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.model_selection import train_test_split\n","\n","# 데이터에서 특성과 타겟 변수를 분리합니다.\n","target = 'matchresult'\n","# Drop identifiers and the original 'tier' feature\n","features_to_drop = ['createdatekst', 'matchid', 'accountid', 'guildid', 'tier', 'matchscore', 'isDrop', 'isEscape']\n","features = processed_data.columns.drop([target] + features_to_drop)\n","X = processed_data[features]\n","y = processed_data[target]\n","\n","# 수치형 및 범주형 데이터를 위한 변환기 정의\n","numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n","categorical_features = X.select_dtypes(include=['object']).columns\n","\n","# 순서형 특성은 이미 'tier_numeric'으로 변환되어 있으므로 여기서는 처리하지 않습니다.\n","\n","# 수치형 및 범주형 데이터에 대한 파이프라인을 만듭니다.\n","numeric_transformer = Pipeline(steps=[\n","    ('imputer', SimpleImputer(strategy='median'))  # NaN 값을 중앙값으로 대체\n","])\n","\n","categorical_transformer = Pipeline(steps=[\n","    ('imputer', SimpleImputer(strategy='most_frequent')),  # NaN 값을 최빈값으로 대체\n","    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # 원-핫 인코딩\n","])\n","\n","# ColumnTransformer를 생성하여 변환기를 결합합니다.\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', numeric_transformer, numeric_features),\n","        ('cat', categorical_transformer, categorical_features)\n","    ],\n","    remainder='passthrough'  # 나머지 열은 변경하지 않고 유지\n",")\n","\n","# 전처리를 특성 데이터에 적용합니다.\n","X_encoded = preprocessor.fit_transform(X)\n","\n","# 데이터셋을 훈련 세트와 검증/테스트 세트로 분할합니다.\n","X_temp, X_test, y_temp, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n","X_train, X_valid, y_train, y_valid = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zkjaWTV75tHN"},"outputs":[],"source":["import torch, gc\n","\n","gc.collect()\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1908686,"status":"ok","timestamp":1698594373989,"user":{"displayName":"Honey Jeong","userId":"03929989360297315294"},"user_tz":-540},"id":"_5GHgZ9eQ7NR","outputId":"bc0b8bcb-9463-424e-eee1-ff1222cdbd6e"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n","  warnings.warn(f\"Device used : {self.device}\")\n"]},{"name":"stdout","output_type":"stream","text":["epoch 0  | loss: 0.68641 | val_0_auc: 0.58574 |  0:08:25s\n","epoch 1  | loss: 0.68401 | val_0_auc: 0.58654 |  0:17:22s\n","Stop training because you reached max_epochs = 2 with best_epoch = 1 and best_val_0_auc = 0.58654\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n","  warnings.warn(wrn_msg)\n"]}],"source":["# from pytorch_tabnet.tab_model import TabNetClassifier\n","\n","# clf = TabNetClassifier()\n","# clf.fit(\n","#     X_train, y_train,\n","#     eval_set=[(X_valid, y_valid)],\n","#     batch_size = 8,\n","#     max_epochs = 2,\n","# )\n","# preds = clf.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":622036,"status":"ok","timestamp":1698595972909,"user":{"displayName":"Honey Jeong","userId":"03929989360297315294"},"user_tz":-540},"id":"esRGsRfDJwZD","outputId":"f5d5cded-00e2-41c0-95e4-477b493834c8"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n","  warnings.warn(f\"Device used : {self.device}\")\n"]},{"name":"stdout","output_type":"stream","text":["epoch 0  | loss: 0.68944 | val_0_auc: 0.57001 |  0:00:12s\n","epoch 1  | loss: 0.67928 | val_0_auc: 0.59253 |  0:00:24s\n","epoch 2  | loss: 0.678   | val_0_auc: 0.59655 |  0:00:36s\n","epoch 3  | loss: 0.67802 | val_0_auc: 0.59728 |  0:00:48s\n","epoch 4  | loss: 0.67714 | val_0_auc: 0.59968 |  0:01:00s\n","epoch 5  | loss: 0.67629 | val_0_auc: 0.60159 |  0:01:12s\n","epoch 6  | loss: 0.67559 | val_0_auc: 0.60327 |  0:01:24s\n","epoch 7  | loss: 0.67462 | val_0_auc: 0.60496 |  0:01:36s\n","epoch 8  | loss: 0.67441 | val_0_auc: 0.60629 |  0:01:48s\n","epoch 9  | loss: 0.67373 | val_0_auc: 0.60862 |  0:02:00s\n","epoch 10 | loss: 0.67488 | val_0_auc: 0.5941  |  0:02:13s\n","epoch 11 | loss: 0.6761  | val_0_auc: 0.60057 |  0:02:25s\n","epoch 12 | loss: 0.67553 | val_0_auc: 0.6047  |  0:02:37s\n","epoch 13 | loss: 0.67401 | val_0_auc: 0.60788 |  0:02:49s\n","epoch 14 | loss: 0.67305 | val_0_auc: 0.60875 |  0:03:01s\n","epoch 15 | loss: 0.67302 | val_0_auc: 0.60912 |  0:03:13s\n","epoch 16 | loss: 0.67268 | val_0_auc: 0.61012 |  0:03:26s\n","epoch 17 | loss: 0.67241 | val_0_auc: 0.61135 |  0:03:38s\n","epoch 18 | loss: 0.67196 | val_0_auc: 0.61219 |  0:03:50s\n","epoch 19 | loss: 0.67174 | val_0_auc: 0.61208 |  0:04:02s\n","epoch 20 | loss: 0.67182 | val_0_auc: 0.612   |  0:04:14s\n","epoch 21 | loss: 0.67192 | val_0_auc: 0.61251 |  0:04:26s\n","epoch 22 | loss: 0.67146 | val_0_auc: 0.61165 |  0:04:38s\n","epoch 23 | loss: 0.67111 | val_0_auc: 0.61255 |  0:04:50s\n","epoch 24 | loss: 0.671   | val_0_auc: 0.61186 |  0:05:02s\n","epoch 25 | loss: 0.67106 | val_0_auc: 0.61343 |  0:05:14s\n","epoch 26 | loss: 0.67074 | val_0_auc: 0.61477 |  0:05:26s\n","epoch 27 | loss: 0.67046 | val_0_auc: 0.61426 |  0:05:38s\n","epoch 28 | loss: 0.67062 | val_0_auc: 0.61396 |  0:05:50s\n","epoch 29 | loss: 0.6705  | val_0_auc: 0.61483 |  0:06:02s\n","epoch 30 | loss: 0.67013 | val_0_auc: 0.61609 |  0:06:14s\n","epoch 31 | loss: 0.66972 | val_0_auc: 0.61626 |  0:06:26s\n","epoch 32 | loss: 0.6699  | val_0_auc: 0.61667 |  0:06:38s\n","epoch 33 | loss: 0.67021 | val_0_auc: 0.61511 |  0:06:50s\n","epoch 34 | loss: 0.66971 | val_0_auc: 0.61666 |  0:07:02s\n","epoch 35 | loss: 0.66969 | val_0_auc: 0.61434 |  0:07:14s\n","epoch 36 | loss: 0.66943 | val_0_auc: 0.61845 |  0:07:26s\n","epoch 37 | loss: 0.66887 | val_0_auc: 0.61857 |  0:07:38s\n","epoch 38 | loss: 0.66933 | val_0_auc: 0.61479 |  0:07:50s\n","epoch 39 | loss: 0.66891 | val_0_auc: 0.61763 |  0:08:02s\n","epoch 40 | loss: 0.66898 | val_0_auc: 0.61706 |  0:08:14s\n","epoch 41 | loss: 0.66907 | val_0_auc: 0.61321 |  0:08:26s\n","epoch 42 | loss: 0.66989 | val_0_auc: 0.61628 |  0:08:38s\n","epoch 43 | loss: 0.66927 | val_0_auc: 0.61873 |  0:08:50s\n","epoch 44 | loss: 0.66954 | val_0_auc: 0.61616 |  0:09:02s\n","epoch 45 | loss: 0.66905 | val_0_auc: 0.6174  |  0:09:14s\n","epoch 46 | loss: 0.6687  | val_0_auc: 0.61875 |  0:09:26s\n","epoch 47 | loss: 0.66814 | val_0_auc: 0.61945 |  0:09:38s\n","epoch 48 | loss: 0.66805 | val_0_auc: 0.61865 |  0:09:50s\n","epoch 49 | loss: 0.66847 | val_0_auc: 0.62004 |  0:10:02s\n","Stop training because you reached max_epochs = 50 with best_epoch = 49 and best_val_0_auc = 0.62004\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n","  warnings.warn(wrn_msg)\n"]}],"source":["# from pytorch_tabnet.tab_model import TabNetClassifier\n","\n","# clf = TabNetClassifier()\n","# clf.fit(\n","#     X_train, y_train,\n","#     eval_set=[(X_valid, y_valid)],\n","#     batch_size = 1024,\n","#     max_epochs = 50,\n","# )\n","# preds = clf.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":2092535,"status":"error","timestamp":1698598388239,"user":{"displayName":"Honey Jeong","userId":"03929989360297315294"},"user_tz":-540},"id":"Ao3oWNvbKkRE","outputId":"74a3d153-be5f-4b29-c7e0-b5f185b72819"},"outputs":[{"name":"stderr","output_type":"stream","text":["[I 2023-10-29 18:05:57,462] A new study created in memory with name: TabNet optimization\n","<ipython-input-13-d0f654435922>:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1)\n","/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n","  warnings.warn(f\"Device used : {self.device}\")\n"]},{"name":"stdout","output_type":"stream","text":["epoch 0  | loss: 0.69493 | valid_accuracy: 0.51916 |  0:00:27s\n","epoch 1  | loss: 0.69092 | valid_accuracy: 0.52235 |  0:00:52s\n","epoch 2  | loss: 0.69007 | valid_accuracy: 0.52492 |  0:01:16s\n","epoch 3  | loss: 0.68971 | valid_accuracy: 0.52523 |  0:01:40s\n"]},{"name":"stderr","output_type":"stream","text":["[W 2023-10-29 18:07:53,398] Trial 0 failed with parameters: {'max_epochs': 14, 'batch_size': 256, 'learning_rate': 0.01673524229404672} because of the following error: KeyboardInterrupt().\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n","    value_or_values = func(trial)\n","  File \"<ipython-input-13-d0f654435922>\", line 19, in objective\n","    model.fit(\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py\", line 258, in fit\n","    self._train_epoch(train_dataloader)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py\", line 489, in _train_epoch\n","    batch_logs = self._train_batch(X, y)\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py\", line 534, in _train_batch\n","    loss.backward()\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 492, in backward\n","    torch.autograd.backward(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 251, in backward\n","    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n","KeyboardInterrupt\n","[W 2023-10-29 18:07:53,401] Trial 0 failed with value None.\n"]},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-d0f654435922>\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# 목적 함수에 대한 최적화 실행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# n_trials는 시도할 횟수입니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# 최적의 하이퍼파라미터 출력\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    449\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \"\"\"\n\u001b[0;32m--> 451\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    452\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     ):\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-d0f654435922>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# 모델 학습\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     model.fit(\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train, eval_set, eval_name, eval_metric, loss_fn, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, from_unsupervised, warm_start, augmentations, compute_importance)\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;31m# Apply predict epoch to all eval sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, train_loader)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py\u001b[0m in \u001b[0;36m_train_batch\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;31m# Perform backward pass and optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_value\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# import optuna\n","# from pytorch_tabnet.tab_model import TabNetClassifier\n","# from sklearn.metrics import accuracy_score\n","\n","# # Optuna를 위한 목적 함수 정의\n","# def objective(trial):\n","\n","#     # 탐색할 하이퍼파라미터 설정\n","#     param = {\n","#         'max_epochs': trial.suggest_int('max_epochs', 10, 30),\n","#         'batch_size': trial.suggest_categorical('batch_size', [256, 512, 1024]),\n","#         'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1)\n","#     }\n","\n","#     # 모델 생성\n","#     model = TabNetClassifier(optimizer_params=dict(lr=param['learning_rate']))\n","\n","#     # 모델 학습\n","#     model.fit(\n","#         X_train, y_train,\n","#         eval_set=[(X_valid, y_valid)],\n","#         eval_name=['valid'],\n","#         eval_metric=['accuracy'],\n","#         max_epochs=param['max_epochs'],\n","#         batch_size=param['batch_size'],\n","#         patience=5,\n","#         virtual_batch_size=128,\n","#         num_workers=0,\n","#         drop_last=False\n","#     )\n","\n","#     # 검증 데이터에 대한 예측 및 정확도 측정\n","#     preds = model.predict(X_valid)\n","#     acc = accuracy_score(y_valid, preds)\n","#     return acc\n","\n","# # Optuna study 객체 생성\n","# study = optuna.create_study(direction='maximize', study_name='TabNet optimization')\n","\n","# # 목적 함수에 대한 최적화 실행\n","# study.optimize(objective, n_trials=20)  # n_trials는 시도할 횟수입니다.\n","\n","# # 최적의 하이퍼파라미터 출력\n","# print(f\"Best trial: {study.best_trial.params}\")\n","# print(f\"Best validation accuracy: {study.best_trial.value}\")\n","\n","# # 최적의 하이퍼파라미터를 사용하여 최종 모델 학습\n","# best_params = study.best_trial.params\n","# best_model = TabNetClassifier(optimizer_params=dict(lr=best_params['learning_rate']))\n","\n","# best_model.fit(\n","#     X_train, y_train,\n","#     eval_set=[(X_valid, y_valid)],\n","#     max_epochs=best_params['max_epochs'],\n","#     batch_size=best_params['batch_size'],\n","#     patience=10,\n","#     virtual_batch_size=128,\n","#     num_workers=0,\n","#     drop_last=False\n","# )\n","\n","# # 테스트 데이터에 대한 최종 평가\n","# final_preds = best_model.predict(X_test)\n","# final_acc = accuracy_score(y_test, final_preds)\n","# print(f\"Test accuracy: {final_acc}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"2CbMBaIqOXFn"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"mount_file_id":"1EoQn0Db3qoiiUGfPNFhylZoxI0ESn7-V","authorship_tag":"ABX9TyN3c/b8bTyY3LW8XZWLRYGG"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}